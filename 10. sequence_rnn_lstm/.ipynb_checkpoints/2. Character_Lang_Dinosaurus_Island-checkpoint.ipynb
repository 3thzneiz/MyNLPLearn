{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aachenosaurus\\nAardonyx\\nAbdallahsaurus\\nAbelisaurus\\nAbrictosaurus\\nAbrosaurus\\nAbydosaurus\\nAcanthopholis\\nAchelousaurus\\nAcheroraptor\\nAchillesaurus\\nAchillobator\\nAcristavus\\nAcrocanthosaurus\\nAcrotholus\\nActiosaurus\\nAdamantisaurus\\nAdasaurus\\nAdelolophus\\nAdeopapposaurus\\nAegyptosaurus\\nAeolosaurus\\nAepisaurus\\nAepyornithomimus\\nAerosteon\\nAetonyxAfromimus\\nAfrovenator\\nAgathaumas\\nAggiosaurus\\nAgilisaurus\\nAgnosphitys\\nAgrosaurus\\nAgujaceratops\\nAgustinia\\nAhshislepelta\\nAirakoraptor\\nAjancingenia\\nAjkaceratops\\nAlamosaurus\\nAlaskacephale\\nAlbalophosaurus\\nAlbertaceratops\\nAlbertadromeus\\nAlbertavenator\\nAlbertonykus\\nAlbertosaurus\\nAlbinykus\\nAlbisaurus\\nAlcovasaurus\\nAlectrosaurus\\nAletopelta\\nAlgoasaurus\\nAlioramus\\nAliwalia\\nAllosaurus\\nAlmas\\nAlnashetri\\nAlocodon\\nAltirhinus\\nAltispinax\\nAlvarezsaurus\\nAlwalkeria\\nAlxasaurus\\nAmargasaurus\\nAmargastegos\\nAmargatitanis\\nAmazonsaurus\\nAmmosaurus\\nAmpelosaurus\\nAmphicoelias\\nAmphicoelicaudia\\nAmphisaurus\\nAmtocephale\\nAmtosaurus\\nAmurosaurus\\nAmygdalodon\\nAnabisetia\\nAnasazisaurus\\nAnatosaurus\\nAnatotitan\\nAnchiceratops\\nAnchiornis\\nAnchisaurus\\nAndesaurus\\nAndhrasaurus\\nAngaturama\\nAngloposeidon\\nAngolatitan\\nAngulomastacator\\nAniksosaurus\\nAnimantarx\\nAnkistrodon\\nAnkylosaurus\\nAnodontosaurus\\nAnoplosaurus\\nAnserimimus\\nAntarctopelta\\nAntarctosaurus\\nAntetonitrus\\nAnthodon\\nAntrodemus\\nAnzu\\nAoniraptor\\nAorun\\nApatodon\\nApatoraptor\\nApatosaurus\\nAppalachiosaurus\\nAquilops\\nAragosaurus\\nAralosaurus\\nAraucanoraptor\\nArchaeoceratops\\nArchaeodontosaurus\\nArchaeopteryx\\nArchaeoraptor\\nArchaeornis\\nArchaeornithoides\\nArchaeornithomimus\\nArcovenator\\nArctosaurus\\nArcusaurus\\nArenysaurus\\nArgentinosaurus\\nArgyrosaurus\\nAristosaurus\\nAristosuchus\\nArizonasaurus\\nArkansaurus\\nArkharavia\\nArrhinoceratops\\nArstanosaurus\\nAsiaceratops\\nAsiamericana\\nAsiatosaurus\\nAstrodon\\nAstrodonius\\nAstrodontaurus\\nAstrophocaudia\\nAsylosaurus\\nAtacamatitan\\nAtlantosaurus\\nAtlasaurus\\nAtlascopcosaurus\\nAtrociraptor\\nAtsinganosaurus\\nAublysodon\\nAucasaurus\\nAugustia\\nAugustynolophus\\nAuroraceratops\\nAurornis\\nAustralodocus\\nAustralovenator\\nAustrocheirus\\nAustroposeidon\\nAustroraptor\\nAustrosaurus\\nAvaceratops\\nAvalonia\\nAvalonianus\\nAviatyrannis\\nAvimimus\\nAvisaurus\\nAvipes\\nAzendohsaurus\\nBactrosaurus\\nBagaceratops\\nBagaraatan\\nBahariasaurus\\nBainoceratops\\nBakesaurus\\nBalaur\\nBalochisaurus\\nBambiraptor\\nBanji\\nBaotianmansaurus\\nBarapasaurus\\nBarilium\\nBarosaurus\\nBarrosasaurus\\nBarsboldia\\nBaryonyx\\nBashunosaurus\\nBasutodon\\nBathygnathus\\nBatyrosaurus\\nBaurutitan\\nBayosaurus\\nBecklespinax\\nBeelemodon\\nBeibeilong\\nBeipiaognathus\\nBeipiaosaurus\\nBeishanlong\\nBellusaurus\\nBelodon\\nBerberosaurus\\nBetasuchus\\nBicentenaria\\nBienosaurus\\nBihariosaurus\\nBilbeyhallorum\\nBissektipelta\\nBistahieversor\\nBlancocerosaurus\\nBlasisaurus\\nBlikanasaurus\\nBolong\\nBonapartenykus\\nBonapartesaurus\\nBonatitan\\nBonitasaura\\nBorealopelta\\nBorealosaurus\\nBoreonykus\\nBorogovia\\nBothriospondylus\\nBrachiosaurus\\nBrachyceratops\\nBrachylophosaurus\\nBrachypodosaurus\\nBrachyrophus\\nBrachytaenius\\nBrachytrachelopan\\nBradycneme\\nBrasileosaurus\\nBrasilotitan\\nBravoceratops\\nBreviceratops\\nBrohisaurus\\nBrontomerus\\nBrontoraptor\\nBrontosaurus\\nBruhathkayosaurus\\nBugenasaura\\nBuitreraptor\\nBurianosaurus\\nBuriolestes\\nByranjaffia\\nByronosaurus\\nCaenagnathasia\\nCaenagnathus\\nCalamosaurus\\nCalamospondylus\\nCalamospondylus\\nCallovosaurus\\nCamarasaurus\\nCamarillasaurus\\nCamelotia\\nCamposaurus\\nCamptonotus\\nCamptosaurus\\nCampylodon\\nCampylodoniscus\\nCanardia\\nCapitalsaurus\\nCarcharodontosaurus\\nCardiodon\\nCarnotaurus\\nCaseosaurus\\nCathartesaura\\nCathetosaurus\\nCaudipteryx\\nCaudocoelus\\nCaulodon\\nCedarosaurus\\nCedarpelta\\nCedrorestes\\nCentemodon\\nCentrosaurus\\nCerasinops\\nCeratonykus\\nCeratops\\nCeratosaurus\\nCetiosauriscus\\nCetiosaurus\\nChangchunsaurus\\nChangdusaurus\\nChangyuraptor\\nChaoyangsaurus\\nCharonosaurus\\nChasmosaurus\\nChassternbergia\\nChebsaurus\\nChenanisaurus\\nCheneosaurus\\nChialingosaurus\\nChiayusaurus\\nChienkosaurus\\nChihuahuasaurus\\nChilantaisaurus\\nChilesaurus\\nChindesaurus\\nChingkankousaurus\\nChinshakiangosaurus\\nChirostenotes\\nChoconsaurus\\nChondrosteosaurus\\nChromogisaurus\\nChuandongocoelurus\\nChuanjiesaurus\\nChuanqilong\\nChubutisaurus\\nChungkingosaurus\\nChuxiongosaurus\\nCinizasaurus\\nCionodon\\nCitipati\\nCladeiodon\\nClaorhynchus\\nClaosaurus\\nClarencea\\nClasmodosaurus\\nClepsysaurus\\nCoahuilaceratops\\nCoelophysis\\nCoelosaurus\\nCoeluroides\\nCoelurosauravus\\nCoelurus\\nColepiocephale\\nColoradia\\nColoradisaurus\\nColossosaurus\\nComahuesaurus\\nComanchesaurus\\nCompsognathus\\nCompsosuchus\\nConcavenator\\nConchoraptor\\nCondorraptor\\nCoronosaurus\\nCorythoraptor\\nCorythosaurus\\nCraspedodon\\nCrataeomus\\nCraterosaurus\\nCreosaurus\\nCrichtonpelta\\nCrichtonsaurus\\nCristatusaurus\\nCrosbysaurus\\nCruxicheiros\\nCryolophosaurus\\nCryptodraco\\nCryptoraptor\\nCryptosaurus\\nCryptovolans\\nCumnoria\\nDaanosaurus\\nDacentrurus\\nDachongosaurus\\nDaemonosaurus\\nDahalokely\\nDakosaurus\\nDakotadon\\nDakotaraptor\\nDaliansaurus\\nDamalasaurus\\nDandakosaurus\\nDanubiosaurus\\nDaptosaurus\\nDarwinsaurus\\nDashanpusaurus\\nDaspletosaurus\\nDasygnathoides\\nDasygnathus\\nDatanglong\\nDatonglong\\nDatousaurus\\nDaurosaurus\\nDaxiatitan\\nDeinocheirus\\nDeinodon\\nDeinonychus\\nDelapparentia\\nDeltadromeus\\nDemandasaurus\\nDenversaurus\\nDeuterosaurus\\nDiabloceratops\\nDiamantinasaurus\\nDianchungosaurus\\nDiceratops\\nDiceratusDiclonius\\nDicraeosaurus\\nDidanodonDilong\\nDilophosaurus\\nDiluvicursor\\nDimodosaurus\\nDinheirosaurus\\nDinodocus\\nDinotyrannus\\nDiplodocus\\nDiplotomodon\\nDiracodon\\nDolichosuchus\\nDollodon\\nDomeykosaurus\\nDongbeititan\\nDongyangopelta\\nDongyangosaurus\\nDoratodon\\nDoryphorosaurus\\nDraconyx\\nDracopelta\\nDracoraptor\\nDracorex\\nDracovenator\\nDravidosaurus\\nDreadnoughtus\\nDrinker\\nDromaeosauroides\\nDromaeosaurus\\nDromiceiomimus\\nDromicosaurus\\nDrusilasaura\\nDryosaurus\\nDryptosauroides\\nDryptosaurus\\nDubreuillosaurus\\nDuriatitan\\nDuriavenator\\nDynamosaurus\\nDyoplosaurus\\nDysalotosaurus\\nDysganus\\nDyslocosaurus\\nDystrophaeus\\nDystylosaurus\\nEchinodon\\nEdmarka\\nEdmontonia\\nEdmontosaurus\\nEfraasia\\nEiniosaurus\\nEkrixinatosaurus\\nElachistosuchus\\nElaltitan\\nElaphrosaurus\\nElmisaurus\\nElopteryx\\nElosaurus\\nElrhazosaurus\\nElvisaurus\\nEmausaurus\\nEmbasaurus\\nEnigmosaurus\\nEoabelisaurus\\nEobrontosaurus\\nEocarcharia\\nEoceratops\\nEocursor\\nEodromaeus\\nEohadrosaurus\\nEolambia\\nEomamenchisaurus\\nEoplophysis\\nEoraptor\\nEosinopteryx\\nEotrachodon\\nEotriceratops\\nEotyrannus\\nEousdryosaurus\\nEpachthosaurus\\nEpanterias\\nEphoenosaurus\\nEpicampodon\\nEpichirostenotes\\nEpidendrosaurus\\nEpidexipteryx\\nEquijubus\\nErectopus\\nErketu\\nErliansaurus\\nErlikosaurus\\nEshanosaurus\\nEuacanthus\\nEucamerotus\\nEucentrosaurus\\nEucercosaurus\\nEucnemesaurus\\nEucoelophysis\\nEugongbusaurus\\nEuhelopus\\nEuoplocephalus\\nEupodosaurus\\nEureodon\\nEurolimnornis\\nEuronychodon\\nEuropasaurus\\nEuropatitan\\nEuropelta\\nEuskelosaurus\\nEustreptospondylus\\nFabrosaurus\\nFalcarius\\nFendusaurus\\nFenestrosaurus\\nFerganasaurus\\nFerganastegos\\nFerganocephale\\nForaminacephale\\nFosterovenator\\nFrenguellisaurus\\nFruitadens\\nFukuiraptor\\nFukuisaurus\\nFukuititan\\nFukuivenator\\nFulengia\\nFulgurotherium\\nFusinasus\\nFusuisaurus\\nFutabasaurus\\nFutalognkosaurus\\nGadolosaurus\\nGaleamopus\\nGalesaurus\\nGallimimus\\nGaltonia\\nGalveosaurus\\nGalvesaurus\\nGannansaurus\\nGansutitan\\nGanzhousaurus\\nGargoyleosaurus\\nGarudimimus\\nGasosaurus\\nGasparinisaura\\nGastonia\\nGavinosaurus\\nGeminiraptor\\nGenusaurus\\nGenyodectes\\nGeranosaurus\\nGideonmantellia\\nGiganotosaurus\\nGigantoraptor\\nGigantosaurus\\nGigantosaurus\\nGigantoscelus\\nGigantspinosaurus\\nGilmoreosaurus\\nGinnareemimus\\nGiraffatitan\\nGlacialisaurus\\nGlishades\\nGlyptodontopelta\\nSkeleton\\nGobiceratops\\nGobisaurus\\nGobititan\\nGobivenator\\nGodzillasaurus\\nGojirasaurus\\nGondwanatitan\\nGongbusaurus\\nGongpoquansaurus\\nGongxianosaurus\\nGorgosaurus\\nGoyocephale\\nGraciliceratops\\nGraciliraptor\\nGracilisuchus\\nGravitholus\\nGresslyosaurus\\nGriphornis\\nGriphosaurus\\nGryphoceratops\\nGryponyx\\nGryposaurus\\nGspsaurus\\nGuaibasaurus\\nGualicho\\nGuanlong\\nGwyneddosaurus\\nGyposaurus\\nHadrosauravus\\nHadrosaurus\\nHaestasaurus\\nHagryphus\\nHallopus\\nHalszkaraptor\\nHalticosaurus\\nHanssuesia\\nHanwulosaurus\\nHaplocanthosaurus\\nHaplocanthus\\nHaplocheirus\\nHarpymimus\\nHaya\\nHecatasaurus\\nHeilongjiangosaurus\\nHeishansaurus\\nHelioceratops\\nHelopus\\nHeptasteornis\\nHerbstosaurus\\nHerrerasaurus\\nHesperonychus\\nHesperosaurus\\nHeterodontosaurus\\nHeterosaurus\\nHexing\\nHexinlusaurus\\nHeyuannia\\nHierosaurus\\nHippodraco\\nHironosaurus\\nHisanohamasaurus\\nHistriasaurus\\nHomalocephale\\nHonghesaurus\\nHongshanosaurus\\nHoplitosaurus\\nHoplosaurus\\nHorshamosaurus\\nHortalotarsus\\nHuabeisaurus\\nHualianceratops\\nHuanansaurus\\nHuanghetitan\\nHuangshanlong\\nHuaxiagnathus\\nHuaxiaosaurus\\nHuaxiasaurus\\nHuayangosaurus\\nHudiesaurus\\nHuehuecanauhtlus\\nHulsanpes\\nHungarosaurus\\nHuxleysaurus\\nHylaeosaurus\\nHylosaurusHypacrosaurus\\nHypselorhachis\\nHypselosaurus\\nHypselospinus\\nHypsibema\\nHypsilophodon\\nHypsirhophus\\nhabodcraniosaurus\\nIchthyovenator\\nIgnavusaurus\\nIguanacolossus\\nIguanodon\\nIguanoides\\nSkeleton\\nIguanosaurus\\nIliosuchus\\nIlokelesia\\nIncisivosaurus\\nIndosaurus\\nIndosuchus\\nIngenia\\nInosaurus\\nIrritator\\nIsaberrysaura\\nIsanosaurus\\nIschioceratops\\nIschisaurus\\nIschyrosaurus\\nIsisaurus\\nIssasaurus\\nItemirus\\nIuticosaurus\\nJainosaurus\\nJaklapallisaurus\\nJanenschia\\nJaxartosaurus\\nJeholosaurus\\nJenghizkhan\\nJensenosaurus\\nJeyawati\\nJianchangosaurus\\nJiangjunmiaosaurus\\nJiangjunosaurus\\nJiangshanosaurus\\nJiangxisaurus\\nJianianhualong\\nJinfengopteryx\\nJingshanosaurus\\nJintasaurus\\nJinzhousaurus\\nJiutaisaurus\\nJobaria\\nJubbulpuria\\nJudiceratops\\nJurapteryx\\nJurassosaurus\\nJuratyrant\\nJuravenator\\nKagasaurus\\nKaijiangosaurus\\nKakuru\\nKangnasaurus\\nKarongasaurus\\nKatepensaurus\\nKatsuyamasaurus\\nKayentavenator\\nKazaklambia\\nKelmayisaurus\\nKemkemiaKentrosaurus\\nKentrurosaurus\\nKerberosaurus\\nKentrosaurus\\nKhaan\\nKhetranisaurus\\nKileskus\\nKinnareemimus\\nKitadanisaurus\\nKittysaurus\\nKlamelisaurusKol\\nKoparion\\nKoreaceratops\\nKoreanosaurus\\nKoreanosaurus\\nKoshisaurus\\nKosmoceratops\\nKotasaurus\\nKoutalisaurus\\nKritosaurus\\nKryptops\\nKrzyzanowskisaurus\\nKukufeldia\\nKulceratops\\nKulindadromeus\\nKulindapteryx\\nKunbarrasaurus\\nKundurosaurus\\nKunmingosaurus\\nKuszholia\\nLabocania\\nLabrosaurus\\nLaelaps\\nLaevisuchus\\nLagerpeton\\nLagosuchus\\nLaiyangosaurus\\nLamaceratops\\nLambeosaurus\\nLametasaurus\\nLamplughsaura\\nLanasaurus\\nLancangosaurus\\nLancanjiangosaurus\\nLanzhousaurus\\nLaosaurus\\nLapampasaurus\\nLaplatasaurus\\nLapparentosaurus\\nLaquintasaura\\nLatenivenatrix\\nLatirhinus\\nLeaellynasaura\\nLeinkupal\\nLeipsanosaurus\\nLengosaurus\\nLeonerasaurus\\nLepidocheirosaurus\\nLepidus\\nLeptoceratops\\nLeptorhynchos\\nLeptospondylus\\nLeshansaurus\\nLesothosaurus\\nLessemsaurus\\nLevnesovia\\nLewisuchus\\nLexovisaurus\\nLeyesaurus\\nLiaoceratops\\nLiaoningosaurus\\nLiaoningtitan\\nLiaoningvenator\\nLiassaurus\\nLibycosaurus\\nLigabueino\\nLigabuesaurus\\nLigomasaurus\\nLikhoelesaurus\\nLiliensternus\\nLimaysaurus\\nLimnornis\\nLimnosaurus\\nLimusaurus\\nLinhenykus\\nLinheraptor\\nLinhevenator\\nLirainosaurus\\nLisboasaurusLiubangosaurus\\nLohuecotitan\\nLoncosaurus\\nLongisquama\\nLongosaurus\\nLophorhothon\\nLophostropheus\\nLoricatosaurus\\nLoricosaurus\\nLosillasaurus\\nLourinhanosaurus\\nLourinhasaurus\\nLuanchuanraptor\\nLuanpingosaurus\\nLucianosaurus\\nLucianovenator\\nLufengosaurus\\nLukousaurus\\nLuoyanggia\\nLurdusaurus\\nLusitanosaurus\\nLusotitan\\nLycorhinus\\nLythronax\\nMacelognathus\\nMachairasaurus\\nMachairoceratops\\nMacrodontophion\\nMacrogryphosaurus\\nMacrophalangia\\nMacroscelosaurus\\nMacrurosaurus\\nMadsenius\\nMagnapaulia\\nMagnamanus\\nMagnirostris\\nMagnosaurus\\nMagulodon\\nMagyarosaurus\\nMahakala\\nMaiasaura\\nMajungasaurus\\nMajungatholus\\nMalarguesaurus\\nMalawisaurus\\nMaleevosaurus\\nMaleevus\\nMamenchisaurus\\nManidens\\nMandschurosaurus\\nManospondylus\\nMantellisaurus\\nMantellodon\\nMapusaurus\\nMarasuchus\\nMarisaurus\\nMarmarospondylus\\nMarshosaurus\\nMartharaptor\\nMasiakasaurus\\nMassospondylus\\nMatheronodon\\nMaxakalisaurus\\nMedusaceratops\\nMegacervixosaurus\\nMegadactylus\\nMegadontosaurus\\nMegalosaurus\\nMegapnosaurus\\nMegaraptor\\nMei\\nMelanorosaurus\\nMendozasaurus\\nMercuriceratops\\nMeroktenos\\nMetriacanthosaurus\\nMicrocephale\\nMicroceratops\\nMicroceratus\\nMicrocoelus\\nMicrodontosaurus\\nMicrohadrosaurus\\nMicropachycephalosaurus\\nMicroraptor\\nMicrovenator\\nMierasaurus\\nMifunesaurus\\nMinmi\\nMinotaurasaurus\\nMiragaia\\nMirischia\\nMoabosaurus\\nMochlodon\\nMohammadisaurus\\nMojoceratops\\nMongolosaurus\\nMonkonosaurus\\nMonoclonius\\nMonolophosaurus\\nMononychus\\nMononykus\\nMontanoceratops\\nMorelladon\\nMorinosaurus\\nMorosaurus\\nMorrosaurus\\nMosaiceratops\\nMoshisaurus\\nMtapaiasaurus\\nMtotosaurus\\nMurusraptor\\nMussaurus\\nMuttaburrasaurus\\nMuyelensaurus\\nMymoorapelta\\nNaashoibitosaurus\\nNambalia\\nNankangia\\nNanningosaurus\\nNanosaurus\\nNanotyrannus\\nNanshiungosaurus\\nNanuqsaurus\\nNanyangosaurus\\nNarambuenatitan\\nNasutoceratops\\nNatronasaurus\\nNebulasaurus\\nNectosaurus\\nNedcolbertia\\nNedoceratops\\nNeimongosaurus\\nNemegtia\\nNemegtomaia\\nNemegtosaurus\\nNeosaurus\\nNeosodon\\nNeovenator\\nNeuquenraptor\\nNeuquensaurus\\nNewtonsaurus\\nNgexisaurus\\nNicksaurus\\nNigersaurus\\nNingyuansaurus\\nNiobrarasaurus\\nNipponosaurus\\nNoasaurus\\nNodocephalosaurus\\nNodosaurus\\nNomingia\\nNopcsaspondylus\\nNormanniasaurus\\nNothronychus\\nNotoceratops\\nNotocolossus\\nNotohypsilophodon\\nNqwebasaurus\\nNteregosaurus\\nNurosaurus\\nNuthetes\\nNyasasaurus\\nNyororosaurus\\nOhmdenosaurus\\nOjoceratops\\nOjoraptorsaurus\\nOligosaurus\\nOlorotitan\\nOmeisaurus\\nOmosaurus\\nOnychosaurus\\nOohkotokia\\nOpisthocoelicaudia\\nOplosaurus\\nOrcomimus\\nOrinosaurusOrkoraptor\\nOrnatotholusOrnithodesmus\\nOrnithoides\\nOrnitholestes\\nOrnithomerus\\nOrnithomimoides\\nOrnithomimus\\nOrnithopsis\\nOrnithosuchus\\nOrnithotarsus\\nOrodromeus\\nOrosaurus\\nOrthogoniosaurus\\nOrthomerus\\nOryctodromeus\\nOshanosaurus\\nOsmakasaurus\\nOstafrikasaurus\\nOstromia\\nOthnielia\\nOthnielosaurus\\nOtogosaurus\\nOuranosaurus\\nOverosaurus\\nOviraptor\\nOvoraptor\\nOwenodon\\nOxalaia\\nOzraptor\\nPachycephalosaurus\\nPachyrhinosaurus\\nPachysauriscus\\nPachysaurops\\nPachysaurus\\nPachyspondylus\\nPachysuchus\\nPadillasaurus\\nPakisaurus\\nPalaeoctonus\\nPalaeocursornis\\nPalaeolimnornis\\nPalaeopteryx\\nPalaeosauriscus\\nPalaeosaurus\\nPalaeosaurus\\nPalaeoscincus\\nPaleosaurus\\nPaludititan\\nPaluxysaurus\\nPampadromaeus\\nPamparaptor\\nPanamericansaurus\\nPandoravenator\\nPanguraptor\\nPanoplosaurus\\nPanphagia\\nPantydraco\\nParaiguanodon\\nParalititan\\nParanthodon\\nPararhabdodon\\nParasaurolophus\\nPareiasaurus\\nParksosaurus\\nParonychodon\\nParrosaurus\\nParvicursor\\nPatagonykus\\nPatagosaurus\\nPatagotitan\\nPawpawsaurus\\nPectinodon\\nPedopenna\\nPegomastax\\nPeishansaurus\\nPekinosaurus\\nPelecanimimus\\nPellegrinisaurus\\nPeloroplites\\nPelorosaurus\\nPeltosaurus\\nPenelopognathus\\nPentaceratops\\nPetrobrasaurus\\nPhaedrolosaurus\\nPhilovenator\\nPhuwiangosaurus\\nPhyllodon\\nPiatnitzkysaurus\\nPicrodon\\nPinacosaurus\\nPisanosaurus\\nPitekunsaurus\\nPiveteausaurus\\nPlanicoxa\\nPlateosauravus\\nPlateosaurus\\nPlatyceratops\\nPlesiohadros\\nPleurocoelus\\nPleuropeltus\\nPneumatoarthrus\\nPneumatoraptor\\nPodokesaurus\\nPoekilopleuron\\nPolacanthoides\\nPolacanthus\\nPolyodontosaurus\\nPolyonax\\nPonerosteus\\nPoposaurus\\nParasaurolophus\\nPostosuchus\\nPowellvenator\\nPradhania\\nPrenocephale\\nPrenoceratops\\nPriconodon\\nPriodontognathus\\nProa\\nProbactrosaurus\\nProbrachylophosaurus\\nProceratops\\nProceratosaurus\\nProcerosaurus\\nProcerosaurus\\nProcheneosaurus\\nProcompsognathus\\nProdeinodon\\nProiguanodon\\nPropanoplosaurus\\nProplanicoxa\\nProsaurolophus\\nProtarchaeopteryx\\nProtecovasaurus\\nProtiguanodon\\nProtoavis\\nProtoceratops\\nProtognathosaurus\\nProtognathus\\nProtohadros\\nProtorosaurus\\nProtorosaurus\\nProtrachodon\\nProyandusaurus\\nPseudolagosuchus\\nPsittacosaurus\\nPteropelyx\\nPterospondylus\\nPuertasaurus\\nPukyongosaurus\\nPulanesaura\\nPycnonemosaurus\\nPyroraptor\\nQantassaurus\\nQianzhousaurus\\nQiaowanlong\\nQijianglong\\nQinlingosaurus\\nQingxiusaurus\\nQiupalong\\nQuaesitosaurus\\nQuetecsaurus\\nQuilmesaurus\\nRachitrema\\nRahiolisaurus\\nRahona\\nRahonavis\\nRajasaurus\\nRapator\\nRapetosaurus\\nRaptorex\\nRatchasimasaurus\\nRativates\\nRayososaurus\\nRazanandrongobe\\nRebbachisaurus\\nRegaliceratops\\nRegnosaurus\\nRevueltosaurus\\nRhabdodon\\nRhadinosaurus\\nRhinorex\\nRhodanosaurus\\nRhoetosaurus\\nRhopalodon\\nRiabininohadros\\nRichardoestesia\\nRileya\\nRileyasuchus\\nRinchenia\\nRinconsaurus\\nRioarribasaurus\\nRiodevasaurus\\nRiojasaurus\\nRiojasuchus\\nRocasaurus\\nRoccosaurus\\nRubeosaurus\\nRuehleia\\nRugocaudia\\nRugops\\nRukwatitan\\nRuyangosaurus\\nSacisaurus\\nSahaliyania\\nSaichania\\nSaldamosaurus\\nSalimosaurus\\nSaltasaurus\\nSaltopus\\nSaltriosaurus\\nSanchusaurus\\nSangonghesaurus\\nSanjuansaurus\\nSanpasaurus\\nSantanaraptor\\nSaraikimasoom\\nSarahsaurus\\nSarcolestes\\nSarcosaurus\\nSarmientosaurus\\nSaturnalia\\nSauraechinodon\\nSaurolophus\\nSauroniops\\nSauropelta\\nSaurophaganax\\nSaurophagus\\nSauroplites\\nSauroposeidon\\nSaurornithoides\\nSaurornitholestes\\nSavannasaurus\\nScansoriopteryx\\nScaphonyx\\nScelidosaurus\\nScipionyx\\nSciurumimus\\nScleromochlus\\nScolosaurus\\nScutellosaurus\\nSecernosaurus\\nSefapanosaurus\\nSegisaurus\\nSegnosaurus\\nSeismosaurus\\nSeitaad\\nSelimanosaurus\\nSellacoxa\\nSellosaurus\\nSerendipaceratops\\nSerikornis\\nShamosaurus\\nShanag\\nShanshanosaurus\\nShantungosaurus\\nShanxia\\nShanyangosaurus\\nShaochilong\\nShenzhousaurus\\nShidaisaurus\\nShingopana\\nShixinggia\\nShuangbaisaurus\\nShuangmiaosaurus\\nShunosaurus\\nShuvosaurus\\nShuvuuia\\nSiamodon\\nSiamodracon\\nSiamosaurus\\nSiamotyrannus\\nSiats\\nSibirosaurus\\nSibirotitan\\nSidormimus\\nSigilmassasaurus\\nSilesaurus\\nSiluosaurus\\nSilvisaurus\\nSimilicaudipteryx\\nSinocalliopteryx\\nSinoceratops\\nSinocoelurus\\nSinopelta\\nSinopeltosaurus\\nSinornithoides\\nSinornithomimus\\nSinornithosaurus\\nSinosauropteryx\\nSinosaurus\\nSinotyrannus\\nSinovenator\\nSinraptor\\nSinusonasus\\nSirindhorna\\nSkorpiovenator\\nSmilodon\\nSonidosaurus\\nSonorasaurus\\nSoriatitan\\nSphaerotholus\\nSphenosaurus\\nSphenospondylus\\nSpiclypeus\\nSpinophorosaurus\\nSpinops\\nSpinosaurus\\nSpinostropheus\\nSpinosuchus\\nSpondylosoma\\nSqualodon\\nStaurikosaurus\\nStegoceras\\nStegopelta\\nStegosaurides\\nStegosaurus\\nStenonychosaurus\\nStenopelix\\nStenotholus\\nStephanosaurus\\nStereocephalus\\nSterrholophus\\nStokesosaurus\\nStormbergia\\nStrenusaurus\\nStreptospondylus\\nStruthiomimus\\nStruthiosaurus\\nStygimoloch\\nStygivenator\\nStyracosaurus\\nSuccinodon\\nSuchomimus\\nSuchosaurus\\nSuchoprion\\nSugiyamasaurus\\nSkeleton\\nSulaimanisaurus\\nSupersaurus\\nSuuwassea\\nSuzhousaurus\\nSymphyrophus\\nSyngonosaurus\\nSyntarsus\\nSyrmosaurus\\nSzechuanosaurus\\nTachiraptor\\nTalarurus\\nTalenkauen\\nTalos\\nTambatitanis\\nTangvayosaurus\\nTanius\\nTanycolagreus\\nTanystropheus\\nTanystrosuchus\\nTaohelong\\nTapinocephalus\\nTapuiasaurus\\nTarascosaurus\\nTarbosaurus\\nTarchia\\nTastavinsaurus\\nTatankacephalus\\nTatankaceratops\\nTataouinea\\nTatisaurus\\nTaurovenator\\nTaveirosaurus\\nTawa\\nTawasaurus\\nTazoudasaurus\\nTechnosaurus\\nTecovasaurus\\nTehuelchesaurus\\nTeihivenator\\nTeinurosaurus\\nTeleocrater\\nTelmatosaurus\\nTenantosaurus\\nTenchisaurus\\nTendaguria\\nTengrisaurus\\nTenontosaurus\\nTeratophoneus\\nTeratosaurus\\nTermatosaurus\\nTethyshadros\\nTetragonosaurus\\nTexacephale\\nTexasetes\\nTeyuwasu\\nThecocoelurus\\nThecodontosaurus\\nThecospondylus\\nTheiophytalia\\nTherizinosaurus\\nTherosaurus\\nThescelosaurus\\nThespesius\\nThotobolosaurus\\nTianchisaurus\\nTianchungosaurus\\nTianyulong\\nTianyuraptor\\nTianzhenosaurus\\nTichosteus\\nTienshanosaurus\\nTimimus\\nTimurlengia\\nTitanoceratops\\nTitanosaurus\\nTitanosaurus\\nTochisaurus\\nTomodon\\nTonganosaurus\\nTongtianlong\\nTonouchisaurus\\nTorilion\\nTornieria\\nTorosaurus\\nTorvosaurus\\nTototlmimus\\nTrachodon\\nTraukutitan\\nTrialestes\\nTriassolestes\\nTribelesodon\\nTriceratops\\nTrigonosaurus\\nTrimucrodon\\nTrinisaura\\nTriunfosaurus\\nTroodon\\nTsaagan\\nTsagantegia\\nTsintaosaurus\\nTugulusaurus\\nTuojiangosaurus\\nTuranoceratops\\nTuriasaurus\\nTylocephale\\nTylosteus\\nTyrannosaurus\\nTyrannotitan\\nIllustration\\nUberabatitan\\nUdanoceratops\\nUgrosaurus\\nUgrunaaluk\\nUintasaurus\\nUltrasauros\\nUltrasaurus\\nUltrasaurus\\nUmarsaurus\\nUnaysaurus\\nUnenlagia\\nUnescoceratops\\nUnicerosaurus\\nUnquillosaurus\\nUrbacodon\\nUtahceratops\\nUtahraptor\\nUteodon\\nVagaceratops\\nVahiny\\nValdoraptor\\nValdosaurus\\nVariraptor\\nVelociraptor\\nVectensia\\nVectisaurus\\nVelafrons\\nVelocipes\\nVelociraptor\\nVelocisaurus\\nVenaticosuchus\\nVenenosaurus\\nVeterupristisaurus\\nViavenator\\nVitakridrinda\\nVitakrisaurus\\nVolkheimeria\\nVouivria\\nVulcanodon\\nWadhurstia\\nWakinosaurus\\nWalgettosuchus\\nWalkeria\\nWalkersaurus\\nWangonisaurus\\nWannanosaurus\\nWellnhoferia\\nWendiceratops\\nWiehenvenator\\nWillinakaqe\\nWintonotitan\\nWuerhosaurus\\nWulagasaurus\\nWulatelong\\nWyleyia\\nWyomingraptor\\nXenoceratops\\nXenoposeidon\\nXenotarsosaurus\\nXianshanosaurus\\nXiaosaurus\\nXingxiulong\\nXinjiangovenator\\nXinjiangtitan\\nXiongguanlong\\nXixianykus\\nXixiasaurus\\nXixiposaurus\\nXuanhanosaurus\\nXuanhuaceratops\\nXuanhuasaurus\\nXuwulong\\nYaleosaurus\\nYamaceratops\\nYandusaurus\\nYangchuanosaurus\\nYaverlandia\\nYehuecauhceratops\\nYezosaurus\\nYibinosaurus\\nYimenosaurus\\nYingshanosaurus\\nYinlong\\nYixianosaurus\\nYizhousaurus\\nYongjinglong\\nYuanmouraptor\\nYuanmousaurus\\nYueosaurus\\nYulong\\nYunganglong\\nYunmenglong\\nYunnanosaurus\\nYunxianosaurus\\nYurgovuchia\\nYutyrannus\\nZanabazar\\nZanclodon\\nZapalasaurus\\nZapsalis\\nZaraapelta\\nZatomusZby\\nZephyrosaurus\\nZhanghenglong\\nZhejiangosaurus\\nZhenyuanlong\\nZhongornis\\nZhongjianosaurus\\nZhongyuansaurus\\nZhuchengceratops\\nZhuchengosaurus\\nZhuchengtitan\\nZhuchengtyrannus\\nZiapelta\\nZigongosaurus\\nZizhongosaurus\\nZuniceratops\\nZunityrannus\\nZuolong\\nZuoyunlong\\nZupaysaurus\\nZuul'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   0: '\\n',\n",
      "    1: 'a',\n",
      "    2: 'b',\n",
      "    3: 'c',\n",
      "    4: 'd',\n",
      "    5: 'e',\n",
      "    6: 'f',\n",
      "    7: 'g',\n",
      "    8: 'h',\n",
      "    9: 'i',\n",
      "    10: 'j',\n",
      "    11: 'k',\n",
      "    12: 'l',\n",
      "    13: 'm',\n",
      "    14: 'n',\n",
      "    15: 'o',\n",
      "    16: 'p',\n",
      "    17: 'q',\n",
      "    18: 'r',\n",
      "    19: 's',\n",
      "    20: 't',\n",
      "    21: 'u',\n",
      "    22: 'v',\n",
      "    23: 'w',\n",
      "    24: 'x',\n",
      "    25: 'y',\n",
      "    26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dWaa\"][1][2] = 10.0\n",
      "gradients[\"dWax\"][3][1] = -10.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [10.]\n",
      "gradients[\"dby\"][1] = [8.45833407]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "dWax = np.random.randn(5,3)*10\n",
    "dWaa = np.random.randn(5,5)*10\n",
    "dWya = np.random.randn(2,5)*10\n",
    "db = np.random.randn(5,1)*10\n",
    "dby = np.random.randn(2,1)*10\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "gradients = clip(gradients, 10)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- python dictionary mapping each character to an index.\n",
    "    seed -- used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)\n",
    "    indices = []\n",
    "    \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "    idx = -1 \n",
    "    \n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
    "    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n",
    "    # trained model), which helps debugging and prevents entering an infinite loop. \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        # for grading purposes\n",
    "        np.random.seed(counter + seed) \n",
    "        \n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "        \n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 0]\n",
      "list of sampled characters: ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', '\\n']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "_, n_a = 20, 100\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "\n",
    "indices = sample(parameters, char_to_ix, 0)\n",
    "print(\"Sampling:\")\n",
    "print(\"list of sampled indices:\", indices)\n",
    "print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Forward propagate through time (≈1 line)\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # Backpropagate through time (≈1 line)\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    # Update parameters (≈1 line)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (100,27) and (71,1) not aligned: 27 (dim 1) != 71 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c591118d1d1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradients[\\\"dWaa\\\"][1][2] =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dWaa\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-aeaff6d1c973>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(X, Y, a_prev, parameters, learning_rate)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Forward propagate through time (≈1 line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Backpropagate through time (≈1 line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyNLPLearn/10. sequence_rnn_lstm/utils.py\u001b[0m in \u001b[0;36mrnn_forward\u001b[0;34m(X, Y, a0, parameters, vocab_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Run one step forward of the RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_step_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Update the loss by substracting the cross-entropy term of this time-step from it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyNLPLearn/10. sequence_rnn_lstm/utils.py\u001b[0m in \u001b[0;36mrnn_step_forward\u001b[0;34m(parameters, a_prev, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mWaa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWya\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Waa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wya'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'by'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0ma_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWaa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mp_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWya\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_next\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# unnormalized log probabilities for next chars # probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (100,27) and (71,1) not aligned: 27 (dim 1) != 71 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "vocab_size, n_a = 27, 100\n",
    "a_prev = np.random.randn(n_a, 1)\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "X = [12,3,5,11,22,3]\n",
    "Y = [4,14,11,22,25, 26]\n",
    "\n",
    "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "print(\"Loss =\", loss)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "print(\"a_last[4] =\", a_last[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "TEST = {}\n",
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    with open(\"dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "#         ### START CODE HERE ###\n",
    "        \n",
    "        # Use the hint above to define one training example (X,Y) (≈ 2 lines)\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        print(X)\n",
    "        print(Y)\n",
    "        print(a_prev)\n",
    "        print(parameters)\n",
    "        return parameters\n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "#         ### END CODE HERE ###\n",
    "        \n",
    "#         # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "#         loss = smooth(loss, curr_loss)\n",
    "\n",
    "#         # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "#         if j % 2000 == 0:\n",
    "            \n",
    "#             print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "#             # The number of dinosaur names to print\n",
    "#             seed = 0\n",
    "#             for name in range(dino_names):\n",
    "                \n",
    "#                 # Sample indices and print them\n",
    "#                 sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "#                 print_sample(sampled_indices, ix_to_char)\n",
    "                \n",
    "#                 seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
    "      \n",
    "#             print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =[None, 20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19]\n",
    "Y =[20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19, 0]\n",
    "a_prev = np.zeros((n_a, 1))\n",
    "parameters\n",
    "curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19]\n",
      "[20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19, 0]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "{'Wax': array([[ 0.01624345, -0.00611756, -0.00528172, ...,  0.00900856,\n",
      "        -0.00683728, -0.0012289 ],\n",
      "       [-0.00935769, -0.00267888,  0.00530355, ..., -0.0035225 ,\n",
      "        -0.01142518, -0.00349343],\n",
      "       [-0.00208894,  0.00586623,  0.00838983, ...,  0.00230095,\n",
      "         0.00762011, -0.00222328],\n",
      "       ...,\n",
      "       [ 0.00219477, -0.01201156, -0.00299095, ...,  0.00107595,\n",
      "         0.00426924, -0.00358589],\n",
      "       [ 0.00603036,  0.00314432,  0.00333115, ...,  0.0066408 ,\n",
      "        -0.01311324,  0.01004093],\n",
      "       [ 0.00873006,  0.01394081, -0.0058878 , ..., -0.00710053,\n",
      "         0.00011437,  0.01430933]]), 'Waa': array([[ 0.01688384,  0.00237324, -0.02498213, ...,  0.00733073,\n",
      "         0.0065218 , -0.00231485],\n",
      "       [ 0.00189199,  0.01223936, -0.00300931, ..., -0.00242826,\n",
      "        -0.00902832,  0.00581509],\n",
      "       [ 0.00857548,  0.00137885,  0.00186075, ..., -0.00209847,\n",
      "         0.01897161, -0.01381391],\n",
      "       ...,\n",
      "       [ 0.00613719, -0.00082253, -0.0014422 , ...,  0.0053007 ,\n",
      "        -0.00638601,  0.01550482],\n",
      "       [ 0.01066143, -0.00689219,  0.00157076, ...,  0.0067524 ,\n",
      "         0.00022452, -0.00871037],\n",
      "       [ 0.00287434, -0.00762185,  0.01070726, ..., -0.00623578,\n",
      "         0.01088111, -0.01939764]]), 'Wya': array([[ 0.01392367, -0.00930314, -0.00993981, ..., -0.01591216,\n",
      "         0.00595189,  0.00273334],\n",
      "       [ 0.00298112, -0.00132637, -0.02251077, ...,  0.02003272,\n",
      "         0.00148823, -0.00551342],\n",
      "       [ 0.01269868,  0.02260748, -0.00182916, ..., -0.00843913,\n",
      "         0.00628342,  0.00537214],\n",
      "       ...,\n",
      "       [-0.00380659, -0.00319029, -0.00707782, ..., -0.00577706,\n",
      "         0.01634844,  0.00895244],\n",
      "       [ 0.02730845, -0.01374647,  0.01767878, ...,  0.01155079,\n",
      "         0.01237279, -0.01880925],\n",
      "       [-0.0134403 , -0.01286985, -0.00783004, ...,  0.00930627,\n",
      "         0.01543065,  0.01381787]]), 'b': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'by': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (50,27) and (71,1) not aligned: 27 (dim 1) != 71 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-7901f49cc189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix_to_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-604e98bb0c0f>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(data, ix_to_char, char_to_ix, num_iterations, n_a, dino_names, vocab_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Choose a learning rate of 0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mcurr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#         ### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-aeaff6d1c973>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(X, Y, a_prev, parameters, learning_rate)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Forward propagate through time (≈1 line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Backpropagate through time (≈1 line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyNLPLearn/10. sequence_rnn_lstm/utils.py\u001b[0m in \u001b[0;36mrnn_forward\u001b[0;34m(X, Y, a0, parameters, vocab_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Run one step forward of the RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_step_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Update the loss by substracting the cross-entropy term of this time-step from it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyNLPLearn/10. sequence_rnn_lstm/utils.py\u001b[0m in \u001b[0;36mrnn_step_forward\u001b[0;34m(parameters, a_prev, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mWaa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWya\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Waa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wya'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'by'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0ma_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWaa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mp_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWya\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_next\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# unnormalized log probabilities for next chars # probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (50,27) and (71,1) not aligned: 27 (dim 1) != 71 (dim 0)"
     ]
    }
   ],
   "source": [
    "parameters = model(data, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
